{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Natural Language Detection\n",
    "\n",
    "In the [Enigma](\"https://github.com/weekendproj/enigma\") solver project, I used [Index of Coincidence](\"https://en.wikipedia.org/wiki/Index_of_coincidence\") as the fitness function to assess us getting \"closer\" to the actual Enigma setting. Higher IoC score implies more \"language-like\" structure which is useful so we know we've actually deciphered a message without actually reading all the billions of messages.\n",
    "\n",
    "Considering how well this very simple frequency-based formula works got me thinking how much better we can get with a little more \"intelligent\" algorithm. This notebook tries a few Machine Learning based approaches.\n",
    "\n",
    "Data collection was rather simple. I used free English ebooks from [Project Gutenberg](\"https://www.gutenberg.org/\") and extracted text from all the `<p></p>` tags (limiting to at least 500 characters to make sure we're getting \"meaty\" texts). These acts as \"perfect\" texts with zero mutations. For every text, we generate mutations by randomly changing a few letters with random ones from the same alphabet (see `mutate` function below)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "import numpy as np\n",
    "\n",
    "data_file = 'data/data.txt'\n",
    "\n",
    "charset_len = 26\n",
    "charset_offset = 65\n",
    "trunc_len = 200\n",
    "mutations_per_string = 15\n",
    "\n",
    "\n",
    "def hist(string):\n",
    "    h = [0 for x in range(charset_len)]\n",
    "    for c in string:\n",
    "        h[c - charset_offset] += 1\n",
    "    return h\n",
    "\n",
    "\n",
    "def ioc(string):\n",
    "    h, n = hist(string), len(string)\n",
    "    numerator = sum([x*x-1 for x in h])\n",
    "    denominator = (n * (n-1)) / charset_len\n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "def mutate(string):\n",
    "    output = []\n",
    "    no_of_mutations = np.random.randint(1, trunc_len+1, mutations_per_string)\n",
    "    \n",
    "    for mut in no_of_mutations:\n",
    "        str_copy = string.copy()\n",
    "        mut_locations = np.random.randint(0, trunc_len, mut)\n",
    "        for loc in mut_locations:\n",
    "            str_copy[loc] = np.random.randint(0, charset_len) + charset_offset\n",
    "        output.append((str_copy, mut))\n",
    "    return output\n",
    "\n",
    "\n",
    "def bigrams(string):\n",
    "    bigrams = [0 for w in range(charset_len**2)]\n",
    "    \n",
    "    for i in range(1, len(string)):\n",
    "        pre = string[i-1] - charset_offset\n",
    "        post = string[i] - charset_offset\n",
    "        \n",
    "        bigrams[pre * charset_len + post] += 1\n",
    "\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "def record(hist, bigrams, score, pct_mutations):\n",
    "    return {\n",
    "        'hist': hist,\n",
    "        'bigrams': bigrams + hist,\n",
    "        'score': score,\n",
    "        'pct_mutations': pct_mutations\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = []\n",
    "with open(data_file, 'r') as file:\n",
    "    for line in file.readlines():\n",
    "        if len(line) < trunc_len:\n",
    "            continue\n",
    "\n",
    "        line = [ord(c) for c in line]\n",
    "        line = line[:trunc_len]\n",
    "        \n",
    "        df.append(record(hist(line), bigrams(mut_string), ioc(line), 0.))\n",
    "        \n",
    "        for mutation in mutate(line):\n",
    "            mut_string, muts = mutation\n",
    "            \n",
    "            df.append(record(hist(mut_string), bigrams(mut_string), ioc(mut_string), muts / trunc_len))\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 27.6 s, sys: 173 ms, total: 27.7 s\n",
      "Wall time: 27.7 s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                hist  \\\n",
       "0  [15, 2, 3, 3, 24, 11, 4, 14, 19, 0, 1, 4, 3, 6...   \n",
       "1  [14, 4, 4, 7, 21, 13, 7, 12, 13, 5, 2, 6, 6, 5...   \n",
       "2  [10, 8, 7, 3, 15, 10, 10, 9, 8, 5, 6, 2, 7, 11...   \n",
       "3  [9, 6, 10, 3, 18, 11, 5, 10, 14, 4, 6, 4, 9, 1...   \n",
       "4  [4, 4, 6, 7, 19, 12, 5, 10, 7, 3, 10, 5, 9, 10...   \n",
       "\n",
       "                                             bigrams     score  pct_mutations  \n",
       "0  [1, 2, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, ...  2.192362          0.000  \n",
       "1  [0, 0, 0, 1, 0, 1, 0, 0, 2, 1, 0, 0, 1, 0, 0, ...  1.377085          0.690  \n",
       "2  [1, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, ...  1.237286          0.890  \n",
       "3  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...  1.279095          0.735  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  1.348342          0.870  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hist</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>score</th>\n",
       "      <th>pct_mutations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[15, 2, 3, 3, 24, 11, 4, 14, 19, 0, 1, 4, 3, 6...</td>\n",
       "      <td>[1, 2, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>2.192362</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[14, 4, 4, 7, 21, 13, 7, 12, 13, 5, 2, 6, 6, 5...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 0, 2, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>1.377085</td>\n",
       "      <td>0.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[10, 8, 7, 3, 15, 10, 10, 9, 8, 5, 6, 2, 7, 11...</td>\n",
       "      <td>[1, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1.237286</td>\n",
       "      <td>0.890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[9, 6, 10, 3, 18, 11, 5, 10, 14, 4, 6, 4, 9, 1...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...</td>\n",
       "      <td>1.279095</td>\n",
       "      <td>0.735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[4, 4, 6, 7, 19, 12, 5, 10, 7, 3, 10, 5, 9, 10...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>1.348342</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As seen below, IoC is pretty well correlated with percent mutation labels in our generated dataset (negative sign because they arre in different direction). We will use this correlation coefficient as the metric to assess usefulness of our subsequent metrics as it's important for this metric to be tightly correlated with \"randomness\" in the string which is expressed by the `pct_mutations` metric in this dataset (we know this as we caused these random mutations)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "df.corr()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  score  pct_mutations\n",
       "score          1.000000      -0.881466\n",
       "pct_mutations -0.881466       1.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>pct_mutations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.881466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pct_mutations</th>\n",
       "      <td>-0.881466</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(np.array(df['hist'].tolist()), df['pct_mutations'].values, test_size=.25, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVR with Frequencies\n",
    "\n",
    "We will start with just character frequencis (same metric IoC is based on) as our featurres. We will use Support Vector Machines as they are good at expanding feature set using kernel functions like polynomials (IoC for example works with squaring every frequency)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "model = SVR(C=.25)\n",
    "model.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SVR(C=0.25)"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "model.score(X_val, y_val)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8830771906346861"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "np.corrcoef(model.predict(X_val), y_val)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.        , 0.94048077],\n",
       "       [0.94048077, 1.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "With 26 features and ~70k training examples, SVR was painfully slow but results are already much better than IoC. We went from ~0.88 to ~0.94 for the correlation coefficient."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(np.array(df['hist'].tolist()), df['pct_mutations'].values, test_size=.25, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "ffnn_model = Sequential()\n",
    "ffnn_model.add(Dense(100, input_dim=26, activation='relu'))\n",
    "for i in range(10):\n",
    "    ffnn_model.add(Dense(100, activation='relu'))\n",
    "\n",
    "ffnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "ffnn_model.compile(loss='mse', optimizer=Adam(learning_rate=10**-3))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=4)\n",
    "\n",
    "ffnn_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=25, batch_size=16, callbacks=[early_stopping])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/25\n",
      "3283/3283 [==============================] - 8s 2ms/step - loss: 0.0126 - val_loss: 0.0113\n",
      "Epoch 2/25\n",
      "3283/3283 [==============================] - 7s 2ms/step - loss: 0.0116 - val_loss: 0.0113\n",
      "Epoch 3/25\n",
      "3283/3283 [==============================] - 7s 2ms/step - loss: 0.0114 - val_loss: 0.0108\n",
      "Epoch 4/25\n",
      "3283/3283 [==============================] - 8s 2ms/step - loss: 0.0112 - val_loss: 0.0120\n",
      "Epoch 5/25\n",
      "3283/3283 [==============================] - 7s 2ms/step - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 6/25\n",
      "3283/3283 [==============================] - 7s 2ms/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 7/25\n",
      "3283/3283 [==============================] - 7s 2ms/step - loss: 0.0111 - val_loss: 0.0108\n",
      "Epoch 8/25\n",
      "3283/3283 [==============================] - 7s 2ms/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 9/25\n",
      "3283/3283 [==============================] - 7s 2ms/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 10/25\n",
      "3283/3283 [==============================] - 7s 2ms/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 11/25\n",
      "3283/3283 [==============================] - 7s 2ms/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 12/25\n",
      "3283/3283 [==============================] - 7s 2ms/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 13/25\n",
      "3283/3283 [==============================] - 7s 2ms/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 14/25\n",
      "3283/3283 [==============================] - 7s 2ms/step - loss: 0.0109 - val_loss: 0.0108\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb2c5f16e10>"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "np.corrcoef(ffnn_model.predict(X_val).T, y_val)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.        , 0.94099866],\n",
       "       [0.94099866, 1.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "residual = np.sum(np.power((ffnn_model.predict(X_val).T - y_val), 2))\n",
    "total = np.sum(np.power((y_val - np.mean(y_val)), 2))\n",
    "\n",
    "1 - (residual / total)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8846960424423719"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(np.array(df['bigrams'].tolist()), df['pct_mutations'].values, test_size=.25, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "%%time\n",
    "\n",
    "model = SVR(C=.25, kernel='poly', degree=2)\n",
    "model.fit(X_train[:30000], y_train[:30000])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 1min 44s, sys: 79 ms, total: 1min 44s\n",
      "Wall time: 1min 44s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SVR(C=0.25, degree=2, kernel='poly')"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "model.score(X_val, y_val)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.922329419918742"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "np.corrcoef(model.predict(X_val), y_val)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.        , 0.96047595],\n",
       "       [0.96047595, 1.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(np.array(df['bigrams'].tolist()), df['pct_mutations'].values, test_size=.25, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "ffnn_model = Sequential()\n",
    "ffnn_model.add(Dense(250, input_dim=len(X_train[0]), activation='relu'))\n",
    "for i in range(20):\n",
    "    ffnn_model.add(Dense(250, activation='relu'))\n",
    "    ffnn_model.add(Dropout(.01))\n",
    "\n",
    "ffnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "ffnn_model.compile(loss='mse', optimizer=Adam(learning_rate=10**-3))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=4)\n",
    "\n",
    "ffnn_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=25, batch_size=16, callbacks=[early_stopping])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/25\n",
      "3283/3283 [==============================] - 29s 8ms/step - loss: 0.0093 - val_loss: 0.0079\n",
      "Epoch 2/25\n",
      "3283/3283 [==============================] - 29s 9ms/step - loss: 0.0074 - val_loss: 0.0067\n",
      "Epoch 3/25\n",
      "3283/3283 [==============================] - 29s 9ms/step - loss: 0.0069 - val_loss: 0.0073\n",
      "Epoch 4/25\n",
      "3283/3283 [==============================] - 29s 9ms/step - loss: 0.0069 - val_loss: 0.0062\n",
      "Epoch 5/25\n",
      "3283/3283 [==============================] - 27s 8ms/step - loss: 0.0066 - val_loss: 0.0064\n",
      "Epoch 6/25\n",
      "3283/3283 [==============================] - 27s 8ms/step - loss: 0.0064 - val_loss: 0.0068\n",
      "Epoch 7/25\n",
      "3283/3283 [==============================] - 28s 9ms/step - loss: 0.0064 - val_loss: 0.0063\n",
      "Epoch 8/25\n",
      "3283/3283 [==============================] - 28s 9ms/step - loss: 0.0061 - val_loss: 0.0074\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb2a5f03a58>"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "residual = np.sum(np.power((ffnn_model.predict(X_val).T - y_val), 2))\n",
    "total = np.sum(np.power((y_val - np.mean(y_val)), 2))\n",
    "\n",
    "1 - (residual / total)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9334016292149889"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "np.corrcoef(ffnn_model.predict(X_val).T, y_val)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.        , 0.96649047],\n",
       "       [0.96649047, 1.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}