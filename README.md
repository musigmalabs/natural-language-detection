# Natural Language Index

In the [Enigma solver]("https://github.com/weekendproj/enigma") project, I used the [Index of Coincidence]("https://en.wikipedia.org/wiki/Index_of_coincidence") as the fitness function to assess us getting "closer" to the actual Enigma setting. Higher IoC score implies more "language-like" structure which is useful so we know we've actually deciphered a message without actually reading all the billions of messages.

Considering how well this very simple frequency-based formula works got me thinking how much better we can get with a little more "intelligent" algorithm. This notebook tries a few Machine Learning approaches. The goal is to maximize the correlation coefficient (I used Pearson coefficient here) of our index with the actual known mutations in an English language string.

The data was generated by extracting English strings from free ebooks from [Project Gutenberg]("https://www.gutenberg.org/") and then randomly mutating them. The books were sampled from different genres and time periods to try to generalize as much as possible. A copy of raw strings is prrovided in the `data` folder.

# Algorithms Tried

I have tried SVM regressor and a feedforward neural networks. FFNNs generally provided better correlation and better performance for inference.

For features, I tried with just 26 letter frequencies first and then also tried bi-grams. Including bi-grams improved the accuracy.

| Features | Algorithm | R squared score | Correlation Coeff |
---------------------------------------------------------------
| Frequencies | IoC | ? | 0.8815 |
| Frequencies | SVM regressor | 0.8831 | 0.9405 |
| Frequencies | FFNN | 0.8847 | 0.9410 |
| Frequencies + Bi-gram | SVM regressor | 0.9223 | 0.9605 |
| Frequencies + Bi-gram | FFNN | 0.9334 | 0.9665 |
